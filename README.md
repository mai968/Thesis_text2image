# TEXT-TO-IMAGE RETRIEVAL FOR MULTIMODAL RECOMMENDATION
The rapid development of artificial intelligence is being applied in many fields. Especially, multimodal image retrieval is gradually becoming a potential research direction, aiming to retrieve images based on user information. Image query researches often allow users to search based on a basic single search method, leading to limitations in searching according to user needs. This thesis proposes a method combining multiple models to enable multimodal image retrieval and evaluate the performance of the research.
The research combines the BLIP model to automatically generate descriptions for images and the CLIP model to map images and text into the same common embedding space. Then, it uses the FAISS library to build an index for searching in the vector space. In addition, to improve query performance, it has been expanded by combining Knowledge Graph technique and Doc2Vec model. Knowledge Graph aims to expand the context of the query, Doc2Vec model supports storing user query information.
The research evaluates the models on the input image dataset. After the research process, the experimental evaluation results show that combining the models and techniques is promising and improves the semantic relevance of the retrieval results. The results return a list of images with the highest similarity to the original query based on cosine similarity. This research shows the effectiveness of multimodal methods and serves as a foundation for future development directions.
